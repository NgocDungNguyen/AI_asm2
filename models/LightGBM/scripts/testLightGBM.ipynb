{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename: /models/LightGBM/scripts/testLightGBM.ipynb\n",
    "\n",
    "# In[0]: IMPORT AND FUNCTIONS\n",
    "#region \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "#endregion\n",
    "\n",
    "# Ensure the necessary directories exist\n",
    "base_dir = \"../../../models/LightGBM\"\n",
    "os.makedirs(f\"{base_dir}/preprocessing\", exist_ok=True)\n",
    "os.makedirs(f\"{base_dir}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{base_dir}/figures\", exist_ok=True)\n",
    "\n",
    "# In[1]: STEP 1. LOAD THE DATA\n",
    "file_path = \"../../../data/NewYork.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# In[2]: HANDLE DATETIME\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data['day_of_year'] = data['datetime'].apply(lambda x: x.timetuple().tm_yday)\n",
    "data['month'] = data['datetime'].dt.month\n",
    "data['hour'] = data['datetime'].dt.hour\n",
    "data['year'] = data['datetime'].dt.year\n",
    "\n",
    "# In[3]: DROP UNNECESSARY COLUMNS\n",
    "columns_to_drop = ['name', 'stations', 'icon', 'description', 'sunrise', 'sunset', 'preciptype', 'snow', 'conditions']\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# In[4]: HANDLE MISSING VALUES\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n",
    "\n",
    "# In[5]: CLUSTERING\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "data['weather_cluster'] = kmeans.fit_predict(data[['temp', 'humidity', 'windspeed', 'precip']])\n",
    "data = data.dropna()\n",
    "\n",
    "# In[6]: CONVERT CATEGORICAL FEATURES TO CATEGORY TYPE\n",
    "cat_feat_names = ['weather_cluster'] \n",
    "\n",
    "if 'weather_cluster' in data.columns:\n",
    "    data[cat_feat_names] = data[cat_feat_names].astype('category')\n",
    "else:\n",
    "    print(\"Warning: 'weather_cluster' column not found. Skipping conversion to category.\")\n",
    "\n",
    "# In[7]: FEATURE ENGINEERING\n",
    "# Modify or add more features based on domain knowledge\n",
    "data['tempmax_lag1'] = data['tempmax'].shift(1)\n",
    "data['tempmax_lag3'] = data['tempmax'].shift(3)\n",
    "data['humidity_lag1'] = data['humidity'].shift(1)\n",
    "data['tempmax_rolling_mean_3'] = data['tempmax'].rolling(window=3).mean()\n",
    "data['tempmax_rolling_std_3'] = data['tempmax'].rolling(window=3).std()\n",
    "data['precip_rolling_sum_7'] = data['precip'].rolling(window=7).sum()\n",
    "\n",
    "# Ensure features are aligned with temperature trends\n",
    "data['day_of_year_cos'] = np.cos(2 * np.pi * data['day_of_year'] / 365)\n",
    "data['day_of_year_sin'] = np.sin(2 * np.pi * data['day_of_year'] / 365)\n",
    "\n",
    "# Fill NaNs only in numeric columns\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n",
    "\n",
    "# Interaction feature\n",
    "data['temp_precip_interaction'] = data['temp'] * data['precip']\n",
    "\n",
    "# In[8]: VISUALIZE RAW DATA\n",
    "# Histograms for each numeric column\n",
    "data.hist(bins=50, figsize=(20, 15))\n",
    "plt.savefig(f\"{base_dir}/figures/hist_raw_data.png\")\n",
    "plt.close()\n",
    "\n",
    "# Scatter plot for a selected feature\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['humidity'], data['tempmax'], alpha=0.2)\n",
    "plt.xlabel('Humidity')\n",
    "plt.ylabel('Maximum Temperature')\n",
    "plt.title('Scatter Plot: Humidity vs Maximum Temperature')\n",
    "plt.savefig(f\"{base_dir}/figures/scatter_1_feat.png\")\n",
    "plt.close()\n",
    "\n",
    "# Scatter matrix for all selected features\n",
    "sns.pairplot(data[[\n",
    "    'tempmax', 'tempmin', 'temp', 'humidity', \n",
    "    'windspeed', 'precip', 'day_of_year_cos', 'day_of_year_sin'\n",
    "]])\n",
    "plt.savefig(f\"{base_dir}/figures/scatter_mat_all_feat.png\")\n",
    "plt.close()\n",
    "\n",
    "# In[9]: MUTUAL INFORMATION FOR FEATURE SELECTION\n",
    "# Drop non-numeric columns before calculating mutual information\n",
    "X = data.drop(columns=['tempmax', 'datetime'])  # Drop the target variable and datetime\n",
    "y = data['tempmax']  # Target variable\n",
    "\n",
    "mi_scores = mutual_info_regression(X, y)\n",
    "mi_scores = pd.Series(mi_scores, index=X.columns)\n",
    "selected_features = mi_scores.sort_values(ascending=False).index[:20]  # Select top 20 features\n",
    "\n",
    "# Retain only selected features\n",
    "X = X[selected_features]\n",
    "data = pd.concat([X, y], axis=1)\n",
    "\n",
    "# In[10]: PIPELINE FOR DATA PROCESSING\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.feature_names]         \n",
    "\n",
    "num_feat_names = [col for col in selected_features if col not in cat_feat_names]\n",
    "cat_feat_names = ['weather_cluster'] if 'weather_cluster' in data.columns else [] \n",
    "\n",
    "# Numerical pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', ColumnSelector(num_feat_names)),\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")), \n",
    "    ('std_scaler', StandardScaler()) \n",
    "])  \n",
    "\n",
    "# Categorical pipeline, if applicable\n",
    "if cat_feat_names:\n",
    "    cat_pipeline = Pipeline([\n",
    "        ('selector', ColumnSelector(cat_feat_names)),\n",
    "        ('imputer', SimpleImputer(strategy=\"most_frequent\")),  # Most frequent for categorical data\n",
    "        ('cat_encoder', OneHotEncoder(sparse_output=False))  # Set sparse_output=False for easier debugging\n",
    "    ])\n",
    "    full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline)\n",
    "    ])\n",
    "else:\n",
    "    full_pipeline = num_pipeline\n",
    "\n",
    "# Process the data\n",
    "processed_data = full_pipeline.fit_transform(data)\n",
    "\n",
    "# Convert processed_data back to DataFrame with appropriate column names\n",
    "processed_columns = num_feat_names\n",
    "if cat_feat_names:\n",
    "    onehot_columns = full_pipeline.transformer_list[1][1].named_steps['cat_encoder'].get_feature_names_out(cat_feat_names)\n",
    "    processed_columns = np.concatenate([num_feat_names, onehot_columns])\n",
    "\n",
    "# Save the processed data\n",
    "processed_data_df = pd.DataFrame(processed_data, columns=processed_columns)\n",
    "processed_data_path = f\"{base_dir}/preprocessing/processed_data.csv\"\n",
    "processed_data_df.to_csv(processed_data_path, index=False)\n",
    "\n",
    "# Save the pipeline\n",
    "pipeline_path = f\"{base_dir}/preprocessing/full_pipeline.pkl\"\n",
    "with open(pipeline_path, 'wb') as file:\n",
    "    pickle.dump(full_pipeline, file)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_data_df, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_path = f\"{base_dir}/preprocessing/X_train.csv\"\n",
    "X_test_path = f\"{base_dir}/preprocessing/X_test.csv\"\n",
    "y_train_path = f\"{base_dir}/preprocessing/y_train.csv\"\n",
    "y_test_path = f\"{base_dir}/preprocessing/y_test.csv\"\n",
    "\n",
    "X_train.to_csv(X_train_path, index=False)\n",
    "X_test.to_csv(X_test_path, index=False)\n",
    "y_train.to_csv(y_train_path, index=False)\n",
    "y_test.to_csv(y_test_path, index=False)\n",
    "\n",
    "# In[11]: TRAIN THE LIGHTGBM MODEL\n",
    "lgbm_model = lgb.LGBMRegressor(objective='regression', random_state=42)\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Save evaluation metrics\n",
    "with open(f\"{base_dir}/results/model_evaluation.txt\", \"w\") as file:\n",
    "    file.write(f\"R2 Score: {r2}\\n\")\n",
    "    file.write(f\"RMSE: {rmse}\\n\")\n",
    "    file.write(f\"MAE: {mae}\\n\")\n",
    "\n",
    "print(f\"R2 Score: {r2}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "# In[12]: HYPERPARAMETER TUNING\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.6, 0.8, 1],\n",
    "    'colsample_bytree': [0.6, 0.8, 1],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0.5, 1, 1.5],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgbm_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Save the best model after tuning\n",
    "best_lgbm_params = random_search.best_params_\n",
    "best_lgbm_model = lgb.LGBMRegressor(**best_lgbm_params, objective='regression', random_state=42)\n",
    "best_lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "model_with_tuning_path = f\"{base_dir}/models/lightgbm_model_with_tuning.pkl\"\n",
    "os.makedirs(os.path.dirname(model_with_tuning_path), exist_ok=True)\n",
    "\n",
    "with open(model_with_tuning_path, 'wb') as file:\n",
    "    pickle.dump(best_lgbm_model, file)\n",
    "\n",
    "print(f\"LightGBM model with hyperparameter tuning saved to {model_with_tuning_path}\")\n",
    "\n",
    "# In[13]: EVALUATE THE TUNED MODEL\n",
    "y_pred_with_tuning = best_lgbm_model.predict(X_test)\n",
    "r2_with_tuning = r2_score(y_test, y_pred_with_tuning)\n",
    "rmse_with_tuning = np.sqrt(mean_squared_error(y_test, y_pred_with_tuning))\n",
    "mae_with_tuning = mean_absolute_error(y_test, y_pred_with_tuning)\n",
    "\n",
    "# Save evaluation metrics after tuning\n",
    "with open(f\"{base_dir}/results/model_evaluation_with_tuning.txt\", \"w\") as file:\n",
    "    file.write(f\"R2 Score: {r2_with_tuning}\\n\")\n",
    "    file.write(f\"RMSE: {rmse_with_tuning}\\n\")\n",
    "    file.write(f\"MAE: {mae_with_tuning}\\n\")\n",
    "\n",
    "print(f\"Evaluation Results for LightGBM model with tuning:\")\n",
    "print(f\"R2 Score: {r2_with_tuning}\")\n",
    "print(f\"RMSE: {rmse_with_tuning}\")\n",
    "print(f\"MAE: {mae_with_tuning}\")\n",
    "\n",
    "# In[14]: FUTURE PREDICTIONS\n",
    "# Reload the original data for creating future predictions\n",
    "original_data = pd.read_csv(file_path)\n",
    "original_data['datetime'] = pd.to_datetime(original_data['datetime'])\n",
    "original_data['day_of_year'] = original_data['datetime'].dt.dayofyear\n",
    "\n",
    "# Generate future dates\n",
    "future_days = 200\n",
    "start_date = pd.to_datetime('today').normalize()\n",
    "future_dates = pd.date_range(start=start_date, periods=future_days, freq='D')\n",
    "\n",
    "# Create base DataFrame for future predictions\n",
    "future_data = pd.DataFrame({'datetime': future_dates})\n",
    "\n",
    "# Prepare historical averages\n",
    "feature_columns = ['tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike', 'humidity', 'windspeed', 'precip']\n",
    "historical_averages = original_data.groupby('day_of_year')[feature_columns].mean().reset_index()\n",
    "\n",
    "# Add 'day_of_year' to future_data\n",
    "future_data['day_of_year'] = future_data['datetime'].dt.dayofyear\n",
    "\n",
    "# Merge to get average feature values for future dates\n",
    "future_data = future_data.merge(historical_averages, on='day_of_year', how='left')\n",
    "\n",
    "# Engineer additional features similar to training data\n",
    "future_data['tempmax_lag1'] = future_data['temp'].shift(1)\n",
    "future_data['tempmax_lag3'] = future_data['temp'].shift(3)\n",
    "future_data['humidity_lag1'] = future_data['humidity'].shift(1)\n",
    "future_data['tempmax_rolling_mean_3'] = future_data['temp'].rolling(window=3).mean()\n",
    "future_data['tempmax_rolling_std_3'] = future_data['temp'].rolling(window=3).std()\n",
    "future_data['precip_rolling_sum_7'] = future_data['precip'].rolling(window=7).sum()\n",
    "future_data['day_of_year_cos'] = np.cos(2 * np.pi * future_data['day_of_year'] / 365)\n",
    "future_data['day_of_year_sin'] = np.sin(2 * np.pi * future_data['day_of_year'] / 365)\n",
    "future_data['temp_precip_interaction'] = future_data['temp'] * future_data['precip']\n",
    "\n",
    "# Ensure all required columns are present (add missing columns with zeros)\n",
    "required_columns = selected_features\n",
    "for col in required_columns:\n",
    "    if col not in future_data.columns:\n",
    "        future_data[col] = 0\n",
    "\n",
    "# Fill missing values in future_data\n",
    "future_data.fillna(method='bfill', inplace=True)\n",
    "future_data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Process future data through the pipeline\n",
    "X_future_processed = full_pipeline.transform(future_data[required_columns])\n",
    "\n",
    "# Make predictions\n",
    "future_tempmax_predictions = best_lgbm_model.predict(X_future_processed)\n",
    "\n",
    "# Compile predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Date': future_data['datetime'],\n",
    "    'Predicted_Max_Temp': future_tempmax_predictions\n",
    "})\n",
    "\n",
    "# Save predictions\n",
    "predictions_df.to_csv(f\"{base_dir}/results/future_tempmax_predictions_200_days.csv\", index=False)\n",
    "print(\"Predictions for the next 200 days saved to ../results/future_tempmax_predictions_200_days.csv\")\n",
    "\n",
    "# Visualization: Plot predictions and actual data\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(predictions_df['Date'], predictions_df['Predicted_Max_Temp'], label='Predicted Max Temp', color='blue', marker='o')\n",
    "plt.scatter(original_data['datetime'], original_data['tempmax'], color='black', alpha=0.5, label='Actual Max Temp', s=10)\n",
    "\n",
    "# Adding confidence intervals (if available)\n",
    "if 'Upper_Bound' in predictions_df.columns and 'Lower_Bound' in predictions_df.columns:\n",
    "    plt.fill_between(predictions_df['Date'], predictions_df['Lower_Bound'], predictions_df['Upper_Bound'], color='blue', alpha=0.2)\n",
    "\n",
    "plt.title('Predicted Maximum Temperature for the Next 200 Days with Actual Data Points')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Predicted Max Temperature (°C)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{base_dir}/figures/future_tempmax_predictions_with_actual.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
